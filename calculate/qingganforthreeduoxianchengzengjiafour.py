import os
import re
import time
from datetime import datetime
from openpyxl import load_workbook
from collections import Counter
import pandas as pd
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import json
import signal

# åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
client = OpenAI(api_key="sk-b56d299a263d4570a59580b1082a262e", base_url="https://api.deepseek.com")

# å…¨å±€å˜é‡
continue_processing = True
lock = threading.Lock()
final_stats = []
RUN_KEYWORD_COUNT = 3  # å¯è®¾ç½®ä¸ºæ•°å­—å¦‚ 5ï¼Œæˆ– "all"


def analyze_sentiment(comment):
    """
    ä½¿ç”¨DeepSeek APIåˆ†æè¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼Œå¹¶æå–æ ¸å¿ƒå…³é”®è¯ã€‚
    :param comment: å¾…åˆ†æçš„è¯„è®ºæ–‡æœ¬
    :return: {'sentiment': 'æ­£é¢'/'è´Ÿé¢', 'keywords': ['å…³é”®è¯1', 'å…³é”®è¯2']} æˆ– None
    """
    try:
        start_time = time.time()

        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system",
                 "content": "ä½ æ˜¯ä¸€ä½æ—…æ¸¸è¯„è®ºæƒ…æ„Ÿåˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£åˆ¤æ–­å°çº¢ä¹¦ä¸Šçš„è¯„è®ºæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢ï¼Œå¹¶æå–å‡ºæœ€èƒ½è¡¨è¾¾æƒ…ç»ªçš„å…³é”®è¯ã€‚"},
                {"role": "user", "content": (
                    "è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†åˆ¤æ–­ä¸€æ¡æ¥è‡ªå°çº¢ä¹¦çš„è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘å¹¶æå–å…³é”®è¯ï¼š\n\n"
                    "1. å¦‚æœè¯„è®ºè¡¨è¾¾äº†å¯¹æ¸¸è§ˆä½“éªŒçš„å–œçˆ±ã€èµç¾æˆ–æ»¡æ„ï¼Œè¯·è¿”å› 'æ­£é¢' å¹¶æå–æœ€èƒ½ä»£è¡¨è¿™ç§æƒ…æ„Ÿçš„å…³é”®è¯ã€‚\n"
                    "2. å¦‚æœè¯„è®ºè¡¨è¾¾äº†ä¸æ»¡ã€æŠ±æ€¨æˆ–æ‰¹è¯„ï¼Œè¯·è¿”å› 'è´Ÿé¢' å¹¶æå–æœ€èƒ½ä»£è¡¨è¿™ç§æƒ…æ„Ÿçš„å…³é”®è¯ã€‚\n"
                    "3. å…³é”®è¯åº”ç®€æ´æ˜äº†ï¼Œä¾‹å¦‚ï¼šå…è´¹ã€æ–¹ä¾¿ã€å‡†ç‚¹ã€äººå¤šã€æ’é˜Ÿã€è´µç­‰ã€‚\n"
                    "4. ä¸è¦è¿”å›æ•´å¥è¯ï¼Œåªæå–å…³é”®è¯å³å¯ã€‚\n"
                    "5. å¦‚æœè¯„è®ºæåˆ°äº†å¤šä¸ªå…³é”®è¯ï¼Œè¯·æŒ‰é‡è¦æ€§æ’åºï¼Œå–æœ€é‡è¦çš„1~2ä¸ªã€‚\n\n"
                    "è¯·ä»¥ä¸¥æ ¼çš„JSONæ ¼å¼è¾“å‡ºç»“æœï¼Œæ ¼å¼ä¸ºï¼š{'sentiment': 'æ­£é¢/è´Ÿé¢', 'keywords': ['å…³é”®è¯1', 'å…³é”®è¯2']}ã€‚\n"
                    "ä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ï¼Œä¸è¦ä½¿ç”¨Markdownæ ¼å¼ã€‚\n\n"
                    f"ç°åœ¨è¯·åˆ†æä»¥ä¸‹è¯„è®ºï¼š\n{comment}"
                )}
            ],
            stream=False
        )

        content = response.choices[0].message.content.strip()
        print(f"ğŸ§  DeepSeekåŸå§‹è¾“å‡º: {content}")

        # âœ… æ·»åŠ å¤„ç†é€»è¾‘ï¼šå°†å•å¼•å·æ›¿æ¢ä¸ºåŒå¼•å·ï¼Œå¹¶ä¿®å¤å¯èƒ½çš„è¯­æ³•é—®é¢˜
        content = content.replace("â€œ", '"').replace("â€", '"').replace("â€˜", "'").replace("â€™", "'")
        content = re.sub(r"(?<!\\)'", '"', content)  # å°†æ‰€æœ‰æœªè½¬ä¹‰çš„å•å¼•å·æ›¿æ¢æˆåŒå¼•å·

        result = json.loads(content)

        sentiment = result.get("sentiment")
        keywords = result.get("keywords", [])

        if not isinstance(keywords, list):
            keywords = []

        elapsed = time.time() - start_time
        print(f"ğŸ” åˆ†æè¯„è®º '{comment[:20]}...' çš„æƒ…æ„Ÿå€¾å‘: {sentiment} | æå–å…³é”®è¯: {keywords}ï¼ˆè€—æ—¶ï¼š{elapsed:.2f}sï¼‰")

        if sentiment in ["æ­£é¢", "è´Ÿé¢"] and keywords:
            return {"sentiment": sentiment, "keywords": keywords}
        else:
            return None

    except json.JSONDecodeError as je:
        print(f"âŒ JSONè§£æå¤±è´¥: {je}ï¼ŒåŸå§‹å†…å®¹ä¸ºï¼š'{content}'")
        return None
    except Exception as e:
        print(f"âŒ è°ƒç”¨DeepSeek APIå¤±è´¥: {e}")
        return None


def process_single_element(element, df_comments, sentiment_stats_map):
    print(f"ğŸ” å¼€å§‹å¤„ç†å…³é”®è¯: '{element}'")

    # ä»sentiment_stats_mapè·å–è¯¥å…³é”®è¯çš„æƒ…æ„Ÿç»Ÿè®¡ä¿¡æ¯
    stats = sentiment_stats_map.get(element, {'æ­£é¢': 0, 'è´Ÿé¢': 0})
    pos_count = stats['æ­£é¢']
    neg_count = stats['è´Ÿé¢']

    total = pos_count + neg_count
    pos_percentage = (pos_count / total * 100) if total > 0 else 0.0
    neg_percentage = (neg_count / total * 100) if total > 0 else 0.0

    # ç­›é€‰å‡ºåŒ…å«å½“å‰å…³é”®è¯çš„æ‰€æœ‰è¯„è®º
    filtered_comments = df_comments[df_comments.iloc[:, 0].astype(str).str.contains(element, case=False, na=False)]

    pos_keywords = set()
    neg_keywords = set()

    for _, row in filtered_comments.iterrows():
        if not continue_processing:
            break

        comment = str(row.iloc[0]).strip()
        result = analyze_sentiment(comment)

        if result:
            sentiment = result["sentiment"]
            keywords = result["keywords"]

            if sentiment == "æ­£é¢":
                pos_keywords.update(keywords)
                if len(pos_keywords) >= 5:
                    pos_keywords = set(list(pos_keywords)[:5])
            elif sentiment == "è´Ÿé¢":
                neg_keywords.update(keywords)
                if len(neg_keywords) >= 5:
                    neg_keywords = set(list(neg_keywords)[:5])

    top_pos = ", ".join(pos_keywords) if pos_keywords else ""
    top_neg = ", ".join(neg_keywords) if neg_keywords else ""

    sentiment_stat = f"æ­£é¢({pos_percentage:.1f}%), è´Ÿé¢({neg_percentage:.1f}%)"

    if top_pos or top_neg:
        result = {
            "æ—…æ¸¸è¦ç´ ": element,
            "æƒ…æ„Ÿå€¾å‘ç»Ÿè®¡": sentiment_stat,
            "ä¸»è¦æ­£é¢æè¿°è¯": top_pos,
            "ä¸»è¦è´Ÿé¢æè¿°è¯": top_neg
        }
        print(f"ğŸ“Š å…³é”®è¯ '{element}' ç»Ÿè®¡å®Œæˆï¼š")
        print(f"   â• æ­£é¢æè¿°è¯ï¼š{top_pos}")
        print(f"   â– è´Ÿé¢æè¿°è¯ï¼š{top_neg}")
        return result
    else:
        print(f"ğŸ—‘ï¸ åˆ é™¤ç©ºè¡Œï¼šå…³é”®è¯ '{element}' çš„ä¸»è¦æ­£/è´Ÿé¢æè¿°è¯å‡ä¸ºç©ºã€‚")
        return None


def read_sentiment_stats(file_path):
    sentiment_stats_map = {}
    try:
        df = pd.read_excel(file_path, sheet_name=0)
        for index, row in df.iterrows():
            element = str(row.iloc[0]).strip()  # ç¬¬ä¸€åˆ—ï¼šå…³é”®è¯
            sentiment = str(row.iloc[2]).strip().lower()  # ç¬¬ä¸‰åˆ—ï¼šæƒ…æ„Ÿå€¾å‘

            if element not in sentiment_stats_map:
                sentiment_stats_map[element] = {'æ­£é¢': 0, 'è´Ÿé¢': 0}

            if sentiment == 'æ­£é¢':
                sentiment_stats_map[element]['æ­£é¢'] += 1
            elif sentiment == 'è´Ÿé¢':
                sentiment_stats_map[element]['è´Ÿé¢'] += 1

        print("ğŸ“Š æƒ…æ„Ÿç»Ÿè®¡æ˜ å°„æ„å»ºå®Œæˆï¼š")
        for k, v in sentiment_stats_map.items():
            print(f"   ğŸ“Œ '{k}': {v}")

    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æƒ…æ„Ÿç»Ÿè®¡æ•°æ®æ–‡ä»¶: {e}")
    return sentiment_stats_map


def extract_red_keywords(file_path):
    red_keywords = []
    try:
        wb = load_workbook(file_path)
        ws = wb.active
        for row in ws.iter_rows():
            for cell in row:
                if cell.font and cell.font.color:
                    color = cell.font.color.rgb
                    if color in ['FFFF0000', 'FF0000']:
                        if cell.value:
                            value = str(cell.value).strip()
                            red_keywords.append(value)
                            print(f"ğŸ”´ æå–åˆ°çº¢è‰²å…³é”®è¯: '{value}'")
        print(f"âœ… æˆåŠŸæå–åˆ° {len(red_keywords)} ä¸ªçº¢è‰²å…³é”®è¯ã€‚")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    return red_keywords


def signal_handler(sig, frame):
    global continue_processing
    print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ä¿¡å·æ•è· (Ctrl+C)ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
    continue_processing = False


def main():
    global continue_processing

    start_time = time.time()
    print(f"ğŸ•’ ç¨‹åºå¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    current_dir = os.path.dirname(os.path.abspath(__file__))
    input_folder = os.path.join(current_dir, '..', 'emotionalInputFile')
    output_folder = os.path.join(current_dir, '..', 'outputfile')

    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    red_file = None
    comments_file = None
    sentiment_file = None

    for f in os.listdir(input_folder):
        if f.startswith("1_") and f.endswith(".xlsx"):
            red_file = f
        elif f.startswith("2_") and f.endswith(".xlsx"):
            sentiment_file = f
        elif f.startswith("3_") and f.endswith(".xlsx"):
            comments_file = f

    if not red_file:
        print("âŒ æœªæ‰¾åˆ°ä»¥ '1_' å¼€å¤´çš„çº¢è‰²å…³é”®è¯æå–æ–‡ä»¶ã€‚")
        return
    if not comments_file:
        print("âŒ æœªæ‰¾åˆ°ä»¥ '3_' å¼€å¤´çš„è¯„è®ºæ–‡ä»¶ã€‚")
        return
    if not sentiment_file:
        print("âŒ æœªæ‰¾åˆ°ä»¥ '2_' å¼€å¤´çš„æƒ…æ„Ÿç»Ÿè®¡æ–‡ä»¶ã€‚")
        return

    file1_path = os.path.join(input_folder, red_file)
    file2_path = os.path.join(input_folder, sentiment_file)
    file3_path = os.path.join(input_folder, comments_file)

    print(f"\nğŸ“„ ä½¿ç”¨æ–‡ä»¶æå–çº¢è‰²å…³é”®è¯: {red_file}")
    print(f"ğŸ“„ ä½¿ç”¨æ–‡ä»¶ä½œä¸ºæƒ…æ„Ÿç»Ÿè®¡æ¥æº: {sentiment_file}")
    print(f"ğŸ“„ ä½¿ç”¨æ–‡ä»¶ä½œä¸ºè¯„è®ºæ¥æº: {comments_file}")

    red_elements = extract_red_keywords(file1_path)
    keywords = [
        "è·èŠ±",
        "æ¹–æ°´",
        "è·å¶",
        "è¥¿æ¹–ç¾æ™¯ä¸‰æœˆå¤©å˜æ˜¥é›¨å¦‚é…’æŸ³å¦‚çƒŸ",
        "å±±è‰²ç©ºè’™é›¨äº¦å¥‡",
        "æ¥å¤©è²å¶æ— ç©·ç¢§",
        "é¢„çº¦",
        "è·¯çº¿",
        "è´¹",
        "é—¨ç¥¨",
        "é…’åº—",
        "æ¥¼å¤–æ¥¼",
        "åœè½¦",
        "æ—¥è½"
    ]

    for keyword in keywords:
        red_elements.append(keyword)
    print(f"ğŸ” æå–åˆ° {len(red_elements)} ä¸ªçº¢è‰²å…³é”®è¯: {red_elements}")
    if not red_elements:
        print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•çº¢è‰²å…³é”®è¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼æˆ–é¢œè‰²è®¾ç½®ã€‚")
        return

    sentiment_stats_map = read_sentiment_stats(file2_path)

    try:
        df_comments = pd.read_excel(file3_path, sheet_name=0)
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–è¯„è®ºæ•°æ®æ–‡ä»¶: {e}")
        return

    unique_elements = set(red_elements)
    if RUN_KEYWORD_COUNT != "all":
        try:
            run_count = int(RUN_KEYWORD_COUNT)
            unique_elements = list(unique_elements)[:run_count]
        except ValueError:
            print("âš ï¸ RUN_KEYWORD_COUNT è®¾ç½®é”™è¯¯ï¼Œé»˜è®¤è·‘å…¨é‡æ•°æ®ã€‚")

    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []
        for element in unique_elements:
            if not continue_processing:
                break
            future = executor.submit(process_single_element, element, df_comments, sentiment_stats_map)
            futures.append(future)

        for future in as_completed(futures):
            if not continue_processing:
                break
            result = future.result()
            if result:
                with lock:
                    final_stats.append(result)

    if final_stats:
        result_df = pd.DataFrame(final_stats, columns=[
            "æ—…æ¸¸è¦ç´ ", "æƒ…æ„Ÿå€¾å‘ç»Ÿè®¡", "ä¸»è¦æ­£é¢æè¿°è¯", "ä¸»è¦è´Ÿé¢æè¿°è¯"
        ])

        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        output_file = os.path.join(output_folder, f"statistics_output_{timestamp}.xlsx")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„: {output_file}")

        try:
            result_df.to_excel(output_file, index=False, engine='openpyxl')
            print(f"âœ… ç»Ÿè®¡ç»“æœå·²æˆåŠŸä¿å­˜è‡³: {output_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    else:
        print("âš ï¸ æ²¡æœ‰å®Œæ•´çš„ç»Ÿè®¡æ•°æ®å¯è¾“å‡ºã€‚")

    total_elapsed = time.time() - start_time
    print(f"ğŸ ç¨‹åºç»“æŸï¼Œæ€»è€—æ—¶ï¼š{total_elapsed:.2f} ç§’")


if __name__ == '__main__':
    main()