import pandas as pd
import jieba
from gensim import corpora, models
import matplotlib.pyplot as plt
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
import webbrowser
from wordcloud import WordCloud
import time
import warnings
import json
import numpy as np
import os

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=RuntimeWarning)

# è®¾ç½®ä¸­æ–‡å­—ä½“ä»¥é¿å…è¯äº‘ä¹±ç 
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False  # æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾


# è‡ªå®šä¹‰ JSON Encoderï¼Œå¤„ç† numpy æ•°æ®ç±»å‹
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, complex):
            return str(obj)
        return super().default(obj)


# 1. è¯»å–Excelæ–‡ä»¶ç¬¬ä¸€ä¸ªsheetçš„ç¬¬ä¸€åˆ—æ•°æ®
def read_excel_comments(file_path):
    df = pd.read_excel(file_path, sheet_name=0)  # ç¬¬ä¸€ä¸ªsheet
    comments = df.iloc[:, 0].dropna().astype(str).tolist()  # ç¬¬ä¸€åˆ—ï¼Œå»é™¤ç©ºå€¼å¹¶è½¬ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
    return comments


# 2. æ–‡æœ¬é¢„å¤„ç†ï¼ˆé€‚ç”¨äºä¸­æ–‡è¯„è®ºï¼‰
def preprocess(texts):
    # åŠ è½½ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆä½ å¯ä»¥æ›¿æ¢ä¸ºä½ è‡ªå·±çš„åœç”¨è¯æ–‡ä»¶è·¯å¾„ï¼‰
    stop_words = set()
    with open("chinese_stopwords.txt", "r", encoding="utf-8") as f:
        for line in f:
            stop_words.add(line.strip())

    processed_texts = []
    for doc in texts:
        tokens = jieba.lcut(doc.lower())  # ä¸­æ–‡åˆ†è¯
        words = [word for word in tokens if word.strip() and word not in stop_words and len(word.strip()) > 1]
        processed_texts.append(words)
    return processed_texts


# 3. æ„å»ºLDAæ¨¡å‹
def run_lda(processed_texts, num_topics):
    dictionary = corpora.Dictionary(processed_texts)
    corpus = [dictionary.doc2bow(text) for text in processed_texts]

    lda_model = models.LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        random_state=42,
        update_every=1,
        chunksize=10,
        passes=5,
        alpha='auto',
        per_word_topics=True
    )

    return lda_model, corpus, dictionary


# 4. æ‰“å°ä¸»é¢˜å…³é”®è¯
def print_topics(lda_model, num_words=10):
    topics = lda_model.print_topics(num_words=num_words)
    for idx, topic in enumerate(topics):
        print(f"\nTopic {idx}:")
        print(topic)


# 5. å¯»æ‰¾æœ€ä½³ä¸»é¢˜æ•°
def find_optimal_number_of_topics(processed_texts, start=2, end=9):  # ä¼˜åŒ–ä¸º 2~9
    coherence_values = []
    model_list = []

    for num_topics in range(start, end + 1):
        print(f"â³ æ­£åœ¨è®­ç»ƒæ¨¡å‹ï¼Œä¸»é¢˜æ•°: {num_topics}")
        start_time = time.time()

        try:
            model, corpus, dictionary = run_lda(processed_texts, num_topics)
            perplexity = -model.log_perplexity(corpus)
            coherence_values.append(perplexity)
            model_list.append((model, corpus, dictionary))
        except Exception as e:
            print(f"âš ï¸ ä¸»é¢˜æ•° {num_topics} è®­ç»ƒå¤±è´¥: {e}")
            continue

        elapsed = time.time() - start_time
        print(f"âœ… ä¸»é¢˜æ•° {num_topics} è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}s")

    x = range(start, end + 1)
    plt.figure()
    plt.plot(x, coherence_values, marker='o')
    plt.xlabel("Num Topics")
    plt.ylabel("-Log Perplexity")
    plt.title("Perplexity vs Number of Topics")
    plt.grid(True)
    plt.savefig("topic_perplexity_curve.png")  # è‡ªåŠ¨ä¿å­˜å›¾åƒ
    plt.close()

    optimal_num_topics = x[coherence_values.index(max(coherence_values))]
    best_model_idx = optimal_num_topics - start
    print(f"âœ… æ¨èçš„æœ€ä½³ä¸»é¢˜æ•°ä¸º: {optimal_num_topics}")
    return model_list[best_model_idx][0], model_list[best_model_idx][1], model_list[best_model_idx][
        2], optimal_num_topics


# 6. å¯è§†åŒ–LDAç»“æœ
def visualize_lda(lda_model, corpus, dictionary, output_file="lda_visual.html"):
    print("ğŸŒ æ­£åœ¨å‡†å¤‡å¯è§†åŒ–æ•°æ®...")
    vis_data = gensimvis.prepare(lda_model, corpus, dictionary, mds="mmds")
    print("ğŸŒ æ­£åœ¨ä¿å­˜å¯è§†åŒ–ç»“æœ...")

    html = pyLDAvis.prepared_data_to_html(vis_data)
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html)

    print(f"ğŸŒ å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    # webbrowser.open(output_file)


# 7. ç”Ÿæˆè¯äº‘ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
def generate_word_clouds(lda_model, num_topics=9, max_words=40, font_path="/Users/guangxin/PycharmProjects/pythonProject1/tongji/SourceHanSans.ttf"):
    # å¦‚æœä½¿ç”¨éç³»ç»Ÿå­—ä½“ï¼Œè¯·æŒ‡å®šå­—ä½“æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ï¼šfont_path="fonts/simhei.ttf"
    for i in range(num_topics):
        print(f"ğŸ¨ æ­£åœ¨ç”Ÿæˆä¸»é¢˜ {i} çš„è¯äº‘...")
        topic_terms = lda_model.show_topic(i, topn=max_words)
        word_freq = {term: freq for term, freq in topic_terms}
        wordcloud = WordCloud(width=800, height=400,
                              background_color='white',
                              font_path=font_path,  # æŒ‡å®šä¸­æ–‡å­—ä½“
                              max_font_size=100,
                              max_words=100,
                              ).generate_from_frequencies(word_freq)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f'ä¸»é¢˜ {i}')
        plt.savefig(f'topic_{i}_wordcloud.png')  # ä¿å­˜å›¾ç‰‡
        plt.close()


# ä¸»å‡½æ•°
def main():
    file_path = "/Users/guangxin/PycharmProjects/pythonProject1/tongji/outputfile/outputResult.xlsx"  # æ›¿æ¢ä¸ºä½ è‡ªå·±çš„è·¯å¾„
    print("ğŸš€ æ­£åœ¨è¯»å–Excelæ–‡ä»¶...")
    comments = read_excel_comments(file_path)

    print("ğŸ§¹ æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬æ•°æ®...")
    processed = preprocess(comments)

    print("ğŸ” æ­£åœ¨å¯»æ‰¾æœ€ä½³ä¸»é¢˜æ•°...")
    best_model, corpus, dictionary, optimal_num_topics = find_optimal_number_of_topics(processed, start=2, end=9)

    print("\nğŸ“Š æå–å‡ºçš„æœ€ä½³ä¸»é¢˜åŠå…³é”®è¯å¦‚ä¸‹ï¼š")
    print_topics(best_model, num_words=10)

    print("ğŸŒ æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–é¡µé¢...")
    visualize_lda(best_model, corpus, dictionary)

    print("â˜ï¸ æ­£åœ¨ç”Ÿæˆè¯äº‘...")
    generate_word_clouds(best_model, num_topics=optimal_num_topics, max_words=40)

    print("âœ… å…¨éƒ¨ä»»åŠ¡å·²å®Œæˆï¼")


if __name__ == "__main__":
    main()
