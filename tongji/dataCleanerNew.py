#   è¯·åœ¨terminalä¸­è¿è¡Œæ­¤è„šæœ¬, pip install -r requirements.txt

import traceback

import pandas as pd
import re
from bs4 import BeautifulSoup
import os
from openai import OpenAI

import time  # æ–°å¢å¯¼å…¥æ—¶é—´æ¨¡å—

# åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
client = OpenAI(api_key="sk-b56d299a263d4570a59580b1082a262e", base_url="https://api.deepseek.com")

# åˆ›å»ºå…¨å±€å˜é‡ds_countï¼Œè®°å½•å¤„ç†çš„æ•°æ®
global ds_count
ds_count = 0
# æ˜¯å¦å¼€å¯DeepSeek,Trueä¸ºå¼€å¯ï¼ŒFalseä¸ºå…³é—­
ds_is_open = False

need_delete = []

# æ— æ•ˆè¯åˆ—è¡¨
sensitive_keywords = {
    'é…·', 'å¥½å¸…', 'èµ›åš', 'å§å§', 'å¸…å“¥', 'å§å¦¹', 'å‰å®³', 'å®å®', 'è”ç³»', 'æ‰‹æœ¯', 'æ¾é¼ æ¡‚é±¼', 'æ¾é¼ ',
    'å§', 'åˆ˜æµ·', 'åˆ˜äº¦è²', 'ä¹°', 'å­¦ç”Ÿ', 'é‹', 'è€å©†', 'è€å…¬', 'åœ°é“', 'æ”¯ä»˜å®', 'å§æ§½', 'ç¾å¥³', 'è¶…æ¨¡', 'æ„Ÿè°¢',
    'ç”·æœ‹å‹', 'ä¸»æ’­', 'èŒ¶å¶', 'éš¾åƒ', 'å¥½å¥½åƒ', 'ç‰¹åˆ«å¥½åƒ', 'è€é¼ ', 'é¥­åº—', 'é¤å…', 'å¥³æœ‹å‹', 'ä½ å¥½', 'é†‹é±¼', 'èŒåœº',
    'æ»¤é•œ', 'ä¸åŒä»·æ ¼', 'åŒ…è£…', 'åˆ«åƒ', 'ä¸å¥½åƒ', 'å•Šå•Šå•Šå•Šå•Šå•Š', 'ä»·æ ¼', 'vä¿¡', 'å¾®ä¿¡', 'è¿™ä¸€åŒ…', 'è¿™ä¸€ç‚¹',
    'ä¸å¤ªå¥½åƒ', 'ä»€ä¹ˆè½¯ä»¶', 'ç¤¼ç›’', 'è‡ªåŠ©é¤', 'åƒä¸äº†', 'å€’æ‰', 'å§¨å¦ˆå·¾', 'è¿™å®¶åº—', 'è¡£å“',
    'å¿«é€’', 'ç‰¹åˆ«å¥½åƒ', 'éå¸¸å¥½å–', 'ç³–é†‹æ’éª¨', 'é±¼æ˜¯é±¼', 'ä½‘åœ£è§‚è·¯', 'ç”µè¯', 'åƒåœ¾é£Ÿå“', 'æ¨æ¢…',
    'ç¬‘æ­»', 'ç¬‘å“­', 'æ·˜å®', 'ä½ ä¿©', 'ç¤¼è²Œæ‹¿å›¾', 'ç¤¼è²Œå–å›¾', 'ä¹Œé²æœ¨é½',
    'è´µä¸è´µ', 'ç½‘çº¢', 'åˆè…¥åˆé…¸', 'åˆé…¸', 'æµªè´¹', 'é±¼é¦™è‚‰ä¸', 'è¿™æ˜¯å“ªå®¶', 'æ˜¨å¤œé›¨ç–é£éª¤', 'æƒ³è¦'
}

CITY_KEYWORDS = {
    # æµ™æ±Ÿçœ
    "å»ºå¾·", "å¯¿æ˜Œ", "æµ·ç›", "å®æ³¢", "è¡¢å·", "æ¸©å·", "ä¸œé˜³", "æ·³å®‰",

    # å¹¿ä¸œçœ
    "æƒ å·", "æ½®å·", "é›·å·", "é˜³æ±Ÿ",

    # æ¹–å—çœ
    "è¡¡é˜³", "è€’é˜³", "æ´¥å¸‚", "éƒ´å·",

    # å››å·çœ
    "è‡ªè´¡", "æˆéƒ½", "ä¹å±±", "å®œå®¾",

    # ç¦å»ºçœ
    "ç¦å·", "å¦é—¨", "æ³‰å·",

    # æ±Ÿè¥¿çœ
    "å—æ˜Œ", "éƒ½æ˜Œ", "ä¸Šé¥¶",

    # æ²³åŒ—çœ
    "çŸ³å®¶åº„", "ä¿å®š",

    # å®‰å¾½çœ
    "é˜œé˜³", "é¢å·è¥¿æ¹–",

    # æ±Ÿè‹çœ
    "æ‰¬å·", "ç˜¦è¥¿æ¹–",

    # å…¶ä»–çœä»½
    "æ¡‚æ—", "å¤©é—¨", "æ­¦æ±‰", "è®¸æ˜Œ", "è¥¿å®‰", "æµå—", "å¤§æ˜æ¹–", "å¤§ç†", "æ²ˆé˜³",

    # æµ·å¤–
    "å±±æ¢¨å¿", "æ²³å†…",
}


def clean_comment(comment):
    if not isinstance(comment, str):
        return ""

    soup = BeautifulSoup(comment, "lxml")
    comment = soup.get_text()

    comment = re.sub(r'https?://\S+|www\.\S+', '', comment)
    comment = re.sub(r'@\S+', '', comment)
    comment = re.sub(r'([^\w\s])\1{2,}', r'\1', comment)
    comment = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s,.!?~ã€ï¼Œã€‚ï¼ï¼Ÿ]', '', comment)

    return comment.strip()


def is_hangzhou_westlake_related(comment):
    global ds_count
    ds_count += 1
    start_timestamp = int(pd.Timestamp.now().timestamp())

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system",
             "content": "ä½ æ˜¯ä¸€ä½æ—…æ¸¸è¯„è®ºåˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£åˆ¤æ–­å°çº¢ä¹¦ä¸Šçš„è¯„è®ºæ˜¯å¦ä¸æ­å·è¥¿æ¹–çš„æ¸¸è§ˆä½“éªŒç›¸å…³ã€‚"},
            {"role": "user", "content": (
                "è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†åˆ¤æ–­ä¸€æ¡æ¥è‡ªå°çº¢ä¹¦çš„è¯„è®ºæ˜¯å¦ä¸æ­å·è¥¿æ¹–çš„æ¸¸è§ˆä½“éªŒç›¸å…³ï¼š\n\n"
                "1. å¦‚æœè¯„è®ºç›´æ¥æè¿°äº†è¥¿æ¹–çš„é£æ™¯ã€æ–‡åŒ–ã€å†å²ã€æ´»åŠ¨ã€æ¸¸è§ˆæ„Ÿå—ç­‰å†…å®¹ï¼Œè¯·è¿”å› 'true'ã€‚\n"
                "2. å¦‚æœè¯„è®ºæ¶‰åŠåˆ°è¥¿æ¹–çš„æ¸¸è§ˆè·¯çº¿ã€æ—¶é—´å®‰æ’ã€äº¤é€šæ¥é©³ã€åœè½¦ã€é—¨ç¥¨ã€å¼€æ”¾æ—¶é—´ç­‰å®ç”¨ä¿¡æ¯ï¼Œè¯·è¿”å› 'true'ã€‚\n"
                "3. å¦‚æœè¯„è®ºä¸»è¦è°ˆè®ºçš„æ˜¯ä¸ªäººå¤–è²Œã€ç¾é£Ÿæ¨èã€éè¥¿æ¹–ç›¸å…³çš„æ—¥å¸¸èŠå¤©ã€å¤©æ°”æŠ±æ€¨ã€ä¸æ¶‰åŠè¥¿æ¹–çš„åœ°é“å…¬äº¤ä¿¡æ¯ç­‰ï¼Œåˆ™è¿”å› 'false'ã€‚\n"
                "4. å¦‚æœè¯„è®ºå†…å®¹æ··åˆäº†è¥¿æ¹–ç›¸å…³ä¿¡æ¯å’Œå…¶ä»–æ— å…³å†…å®¹ï¼Œè¯·å°è¯•æå–æ ¸å¿ƒæ„å›¾ï¼Œè‹¥ä¸»è¦å…³æ³¨ç‚¹åœ¨è¥¿æ¹–æ¸¸è§ˆä½“éªŒä¸Šï¼Œè¯·è¿”å› 'true'ã€‚\n"
                "5. ç‰¹åˆ«æ³¨æ„ï¼šå¦‚æœè¯„è®ºä¸­çš„â€˜å…¬äº¤â€™â€˜åœ°é“â€™â€˜æ‰“è½¦â€™ç­‰è¯æ±‡æ˜¯ä¸ºäº†äº†è§£å¦‚ä½•æ›´å¥½åœ°æ¸¸è§ˆè¥¿æ¹–æˆ–å…¶å‘¨è¾¹æ™¯ç‚¹ï¼Œè¯·è§†ä¸ºç›¸å…³å†…å®¹å¹¶è¿”å› 'true'ã€‚\n\n"
                "è¯·åªè¾“å‡ºä¸€ä¸ªå•è¯ï¼š'true' æˆ– 'false'ï¼Œä¸è¦æ·»åŠ å…¶ä»–è§£é‡Šæˆ–æ ¼å¼ã€‚\n\n"
                "ç°åœ¨è¯·åˆ¤æ–­ä»¥ä¸‹è¯„è®ºæ˜¯å¦ä¸è¥¿æ¹–æ¸¸è§ˆä½“éªŒç›¸å…³ï¼š\n"
                f"{comment}"
            )}
        ],
        stream=False
    )

    end_timestamp = int(pd.Timestamp.now().timestamp())
    print(
        f'æ­£åœ¨å¤„ç†ç¬¬{ds_count}æ¡æ•°æ®ï¼Œè¯„è®ºå†…å®¹ï¼š{comment[:30]}...ï¼Œdeepseekå¤„ç†ç»“æœï¼š{response.choices[0].message.content},è€—æ—¶ï¼š' + str(
            end_timestamp - start_timestamp) + 'ç§’')

    answer = response.choices[0].message.content.strip().lower()
    return answer == 'true'


def contains_ad(comment):
    if 'ä¼˜æƒ ' in comment:
        return True
    if re.search(r'ç§ä¿¡.*?([a-zA-Z0-9_\-]{5,}|[0-9]{6,})', comment):
        return True
    return False


def is_meaningful_chinese(text, min_chinese_chars=5):
    chinese_chars = re.findall(r'[\u4e00-\u9fa5]', text)
    if len(chinese_chars) < min_chinese_chars:
        return False

    common_gibberish = ['å“ˆå“ˆ', 'å‘µå‘µ', 'å˜»å˜»', 'å“¦å“¦', 'å—¯å—¯', 'å˜¿å˜¿', 'é¹…é¹…']
    for word in common_gibberish:
        if text.startswith(word * 2):  # å¦‚ "å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆ"
            return False

    if len(text) > 3 * len(chinese_chars):
        return False

    return True


def has_repeated_pattern(text, repeat_threshold=3):
    words = re.split(r'\s+', text)
    from collections import Counter
    counter = Counter(words)
    for word, count in counter.items():
        if count >= repeat_threshold and len(word) >= 2:
            return True
    return False


def contains_city_keyword(comment):
    for city in CITY_KEYWORDS:
        if city in comment:
            return True
    return False


def load_exclusion_keywords(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        keywords = [line.strip() for line in f.readlines()]
    return set(keywords)


def filter_comments(input_file, output_file, exclusion_file=None):
    if os.path.isfile(output_file):
        os.remove(output_file)

    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    df = pd.read_excel(input_file, sheet_name=0)
    comments = df.iloc[:, 0].dropna().astype(str)

    filtered_comments = []
    exclusion_keywords = set()
    if exclusion_file and os.path.isfile(exclusion_file):
        exclusion_keywords = load_exclusion_keywords(exclusion_file)

    for comment in comments:
        cleaned = clean_comment(comment)

        if cleaned in filtered_comments:
            continue

        if not is_meaningful_chinese(cleaned):
            continue

        if has_repeated_pattern(cleaned):
            continue

        if exclusion_keywords:
            for word in exclusion_keywords:
                cleaned = re.sub(r'\b' + re.escape(word) + r'\b', '', cleaned)
            cleaned = re.sub(r'\s+', ' ', cleaned).strip()

        if len(cleaned) < 5 and 'è¥¿æ¹–' not in cleaned:
            continue

        if contains_city_keyword(cleaned):
            continue

        if any(kw in cleaned for kw in sensitive_keywords):
            continue

        if contains_ad(cleaned):
            continue

        if ds_is_open:
            if not is_hangzhou_westlake_related(cleaned):
                continue

        filtered_comments.append(cleaned)
        print(f"ä¿ç•™è¯„è®ºï¼š{cleaned}")

    result_df = pd.DataFrame(filtered_comments, columns=["Cleaned Comments"])
    result_df.to_excel(output_file, index=False)
    print(f"å¤„ç†å®Œæˆï¼Œå…±ä¿ç•™ {len(filtered_comments)} æ¡æœ‰æ•ˆè¯„è®ºï¼Œå·²ä¿å­˜è‡³ {output_file}")


def main():
    start_time = time.time()
    current_dir = os.path.dirname(os.path.abspath(__file__))

    input_folder = os.path.join(current_dir, '..', 'inputFolder')
    output_folder = os.path.join(current_dir, '..', 'outputfile')
    exclude_path = os.path.join(current_dir, 'deletewords.txt')

    # åˆ›å»ºè¾“å…¥è¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists(input_folder):
        os.makedirs(input_folder)
        print("ğŸ“ è¾“å…¥æ–‡ä»¶å¤¹å·²è‡ªåŠ¨åˆ›å»ºï¼Œè¯·å°†å¾…å¤„ç†çš„ .xlsx æ–‡ä»¶æ”¾å…¥ä»¥ä¸‹ç›®å½•åé‡æ–°è¿è¡Œç¨‹åºï¼š")
        print(os.path.abspath(input_folder))
        return

    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰ .xlsx æ–‡ä»¶
    xlsx_files = [f for f in os.listdir(input_folder) if f.endswith(".xlsx")]
    if not xlsx_files:
        print("âš ï¸ è¾“å…¥æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰å¯å¤„ç†çš„ .xlsx æ–‡ä»¶ã€‚")
        print("ğŸ“‚ è¯·å°†å¾…å¤„ç†çš„ .xlsx æ–‡ä»¶æ”¾å…¥ä»¥ä¸‹ç›®å½•ï¼š")
        print(os.path.abspath(input_folder))
        return

    print(f"âœ… ç¨‹åºå¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}")
    print(f"ğŸ” æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹: {os.path.abspath(input_folder)}")

    total_files = 0
    success_files = 0

    for filename in xlsx_files:
        total_files += 1
        input_path = os.path.join(input_folder, filename)
        output_filename = f"outputResult_{filename}"
        output_path = os.path.join(output_folder, output_filename)

        try:
            print(f"\nğŸ“¦ æ­£åœ¨å¤„ç†æ–‡ä»¶: {filename}")
            filter_comments(input_path, output_path, exclude_path)
            success_files += 1
            print(f"âœ… æ–‡ä»¶ {filename} å¤„ç†å®Œæˆã€‚")
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ {filename} å¤„ç†å¤±è´¥ï¼é”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
            traceback.print_exc()  # æ‰“å°å®Œæ•´é”™è¯¯å †æ ˆ

    end_time = time.time()
    elapsed_time = end_time - start_time

    print("\n" + "=" * 60)
    print(f"ğŸ ç¨‹åºç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}")
    print(f"â±ï¸ æ€»å…±è€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"ğŸ“„ å…±å¤„ç†æ–‡ä»¶æ•°é‡: {total_files}")
    print(f"âœ… æˆåŠŸå¤„ç†æ–‡ä»¶æ•°é‡: {success_files}")
    print(f"âŒ å¤±è´¥å¤„ç†æ–‡ä»¶æ•°é‡: {total_files - success_files}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {os.path.abspath(output_folder)}")
    print("=" * 60)


if __name__ == '__main__':
    main()
